The purpose of this epic is to enable a Dynamic Ingest client to create new renditions from a previously-retrieved and archived digital master rendition. Current requests to the Dynamic Ingest API are of the form:

POST /v1/accounts/1234/videos/5678/ingest-requests
{
    "master": { "url": "http://host/file.mp4" },
    "profile": "videocloud-default-v1"
}

This new API permits requests that use a video’s archived digital master as the source of additional renditions, by specifying the a boolean JSON attribute “use-archived-master”, set to true, in place of the URL of the external master:

POST /v1/accounts/1234/videos/5678/ingest-requests
{
    "master": { "use_archived_master": true },
    "profile": "my-new-profile"
}

Requests specifying “use-archived-master” as false without also specifying a “url” attribute are invalid. Requests with both a master URL and use-archive-master set to true are also invalid.

The resulting ingestion request made to the Mannheim ingestion stack replaces the “source” attribute with a “use-digital-master-as-source” boolean attribute:

POST /private/initializetemplatejobfromorigin/template/ingest
{
    "publisherId": 1234,
"videoId": 5678,
“source”: “”,
    "useDigitalMasterAsSource": true,
    "ingestProfile": {
"id": "54ee5b64e4b0a6d2bf9cb163",
"version": 1424907108122,
"name": "my-new-profile",
"renditions": [
            {
…
}

The generated Mannheim DSL uses two new workitem types to fetch the archived master for use by other workers:


{
"digitalmastergroup": [
    {
       “digitalMasterCoordinates”: {
          “type”: “fetchdigitalmasterinfo”,
          “publisherId”: 1234,
             “videoId”: 5678
         }
},
{
   “digitalMasterURI”: {
“type”: “bcfscoordinatestouri”,
“fileStore”: “$digitalMasterCoordinates.bcfsFileStore”
“key”: “$fetchDigitalMasterInfo.bcfsKey”
       }
    },
        {
   "onvalidatereject": {
                "type": "jobstate",
"jobstate": "EXCEPTION",
"conditions": [
   "&validate.rejected"
                ]
   }
        },
        {
   "digitalmaster": {
"type": "checkpoint",
"conditions": [
                   "&validate.complete"
                ]
            }
          }
          ]
      },
    ...
      {
   "encoding": [
          {
   "zensubmit": {
"workitemFormat": "zencoder",
                "source": “$digitalMasterURI.masterURI”,

...

The fetchDigitalMasterInfo worker queries Roebuck to get the BCFS coordinates of the archived digital master.

The prepareDigitalMaster worker makes the archived digital master available to Zencoder. It does this one of 3 ways:

Prepare a signed S3 URL
Copy it to a less-risky S3 bucket with a short TTL
Sends specific S3 credentials along in the request to Zencoder


Notes from Grooming 13-Aug
Customer documentation should be updated for custom profiles suggesting that you add your master, since that is not the default
Retranscode priority should be set to “2” (or something higher than 1)
Will there be a separate API for retranscode and ingest?
No, just a new flag in the “master” section of the existing public-facing DI request
Should this effort include moving our validate worker in to DI?
No, we should keep this the same as DI
There are things we should validate, but we can do it from metadata
Do we intend to support DI retranscodes from non-DI uploads?
No, not publicly.
But it will work if the master is in S3.
Do we intend to support DI retranscodes from non-S3 archives?
No.  We will have technical support for anything in S3 archives, only.
But if we add it later, we would handle this in the “prepare digital master” step
How does a “there is no master” exception work?
Async callback as part of the “failvideo” callback (after the fetch digital master info worker)
Maybe in wedge - discussion for another day
Does swaprenditions still happen?
Yes, atomically and only if all renditions are processed successfully.
Schedule follow up meeting to talk about DM security issues
Make sure we don’t stomp the DM on retranscode, no matter what


Notes Aug-17 - on how to do ZC-to-DM-archive creds
Pre-signed URLs sent to ZC
URL that could be accessible to anyone who could see it
Apply the ZC policy to the bucket
Potentially opens up content to other ZC customers (if they can guess our URLs)
Store ZC-specific credentials that can access the bucket in ZC
Works and is secure today, but would require some API to maintain creds when we move to one-ZC-account-to-one-VC-account
This option is selected.  The work around handling this issue for multiple accounts.
To keep things working the same way they are today:
Don’t make the attached creds the “default for S3”
For requests that need the creds, add the named credential to the request
Copy to a scratch bucket
Takes time
No more secure than allowing access to the whole bucket, but less surface exposed
Have a per-publisher IAM credential that has access to the DM bucket, but only with that publisher’s prefix
Major maintenance issue
Way overcomplicated for today’s problem
